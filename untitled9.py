# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zEm-GPLXm_MmtaRdkt1vfjbwkEZTVSvB
"""

"""
forecast_lstm_mc_dropout_optuna.py

Advanced Time Series Forecasting with LSTM + Optuna hyperparameter tuning
and Monte Carlo Dropout uncertainty quantification.
Baseline comparison: SARIMA (pmdarima)

Usage:
  1) Install required packages:
       pip install numpy pandas matplotlib scikit-learn torch torchvision optuna pmdarima kaggle tqdm seaborn
     (If you prefer TensorFlow/Keras, this script uses PyTorch to keep MC Dropout control simple.)

  2) Make sure Kaggle API credentials file `kaggle.json` is at ~/.kaggle/kaggle.json
     or set environment variables as per Kaggle docs.

  3) Run:
       python forecast_lstm_mc_dropout_optuna.py

Notes:
 - Dataset used: "Daily Climate time series data" from Kaggle (Delhi). This is multivariate and > 500 daily observations.
 - Uncertainty: Monte Carlo Dropout â€” run model in train() mode at inference for multiple stochastic forward passes and compute mean/intervals.
"""

import os
import sys
import math
from pathlib import Path
import tempfile
import zipfile
import random
import argparse
import json
from tqdm import tqdm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import optuna
import pmdarima as pm

# ---------------------------
# Configuration
# ---------------------------
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
torch.manual_seed(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

KAGGLE_DATASET = "sumanthvrao/daily-climate-time-series-data"  # Kaggle dataset id
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# ---------------------------
# Helper: download Kaggle dataset programmatically
# ---------------------------
def download_kaggle_dataset(dataset: str, target_dir: Path):
    """Download dataset from kaggle using kaggle API CLI (requires kaggle.json credentials)."""
    print(f"Downloading Kaggle dataset {dataset} into {target_dir} ...")
    target_dir.mkdir(parents=True, exist_ok=True)
    zip_path = target_dir / "kaggle_dataset.zip"
    # Use Kaggle CLI if available
    cmd = f"kaggle datasets download -d {dataset} -p {str(target_dir)} --force"
    ret = os.system(cmd)
    if ret != 0:
        raise RuntimeError("kaggle CLI download failed. Ensure kaggle.json and `kaggle` CLI installed.")
    # Unzip files
    # Kaggle CLI writes zipped file with name dataset.zip
    # find .zip in target_dir
    zips = list(target_dir.glob("*.zip"))
    if not zips:
        print("No zip found; perhaps dataset already present.")
        return
    z = zips[0]
    with zipfile.ZipFile(z, 'r') as zip_ref:
        zip_ref.extractall(target_dir)
    print("Downloaded and extracted.")
    return

# ---------------------------
# Load dataset
# ---------------------------
def load_daily_delhi_climate(data_dir: Path):
    """Load Daily Delhi Climate dataset (Kaggle). Expect file DailyDelhiClimateTrain.csv or similar."""
    # Try common filenames
    candidates = [
        data_dir / "DailyDelhiClimateTrain.csv",
        data_dir / "DailyDelhiClimate.csv",
        data_dir / "DailyDelhiClimateTrain.csv",
        data_dir / "DailyDelhiClimateTest.csv",
        data_dir / "DailyDelhiClimateTrain.csv"
    ]
    found = None
    for c in candidates:
        if c.exists():
            found = c
            break
    if not found:
        # try listing CSVs
        csvs = list(data_dir.glob("*.csv"))
        if csvs:
            found = csvs[0]
    if not found:
        raise FileNotFoundError("Could not find dataset CSV in data directory. Download dataset from Kaggle first.")
    print("Using dataset file:", found)
    df = pd.read_csv(found)
    # typical Kaggle file has columns: date, meantemp, humidity, wind_speed, meanpressure or similar
    # Ensure date column exists
    date_col = None
    for col in df.columns:
        if 'date' in col.lower():
            date_col = col
            break
    if date_col is None:
        raise RuntimeError("No date column detected in CSV.")
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values(date_col).reset_index(drop=True)
    df = df.set_index(date_col)
    # Keep numeric columns only
    df = df.select_dtypes(include=[np.number])
    if df.shape[0] < 500:
        raise RuntimeError("Dataset has fewer than 500 observations; please choose different dataset.")
    print("Dataset shape (rows x cols):", df.shape)
    return df

# ---------------------------
# Sequence dataset for LSTM
# ---------------------------
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        # X: (n_samples, seq_len, n_features)
        # y: (n_samples, horizon) or (n_samples,)
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

def create_sequences(df_values, seq_len=30, horizon=1):
    """
    df_values: numpy array (T, n_features)
    returns X (N, seq_len, n_features), y (N, horizon)
    """
    T, n_feat = df_values.shape
    X, y = [], []
    for i in range(T - seq_len - horizon + 1):
        X.append(df_values[i:i+seq_len])
        y.append(df_values[i+seq_len:i+seq_len+horizon, 0])  # predict first variable (e.g., mean temp)
    X = np.array(X)
    y = np.array(y)
    if y.shape[1] == 1:
        y = y.reshape(-1)
    return X, y

# ---------------------------
# Model definition: LSTM with dropout
# ---------------------------
class LSTM_MC_Dropout(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2, fc_units=64):
        super().__init__()
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            dropout=dropout if num_layers>1 else 0.0)
        # Use dropout layer explicitly for MC Dropout (applied to outputs)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, fc_units),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(fc_units, 1)
        )

    def forward(self, x):
        # x: (B, seq_len, input_size)
        out, (h_n, c_n) = self.lstm(x)  # out: (B, seq_len, hidden_size)
        # take last time step
        last = out[:, -1, :]  # (B, hidden_size)
        dropped = self.dropout(last)
        return self.fc(dropped).squeeze(-1)  # (B,)

# ---------------------------
# Train / Eval utilities
# ---------------------------
def train_one_epoch(model, loader, opt, loss_fn):
    model.train()
    total_loss = 0.0
    for xb, yb in loader:
        xb = xb.to(DEVICE)
        yb = yb.to(DEVICE)
        opt.zero_grad()
        preds = model(xb)
        loss = loss_fn(preds, yb)
        loss.backward()
        opt.step()
        total_loss += loss.item() * xb.size(0)
    return total_loss / len(loader.dataset)

def evaluate_model(model, loader, mc_dropout_samples=1):
    model.eval()
    preds = []
    trues = []
    # For MC Dropout: set model to train mode AND still disable gradient
    if mc_dropout_samples > 1:
        model.train()  # enable dropout
        with torch.no_grad():
            for xb, yb in loader:
                xb = xb.to(DEVICE)
                # generate multiple stochastic predictions per batch
                batch_preds = []
                for _ in range(mc_dropout_samples):
                    p = model(xb).cpu().numpy()
                    batch_preds.append(p)
                batch_preds = np.stack(batch_preds, axis=0)  # (mc, B)
                preds.append(batch_preds)  # append arrays to list
                trues.append(yb.numpy())
        # concat over batches
        preds = np.concatenate([p for p in preds], axis=1)  # (mc, N)
        preds = preds.transpose(1,0)  # (N, mc)
        trues = np.concatenate(trues, axis=0)  # (N,)
        return preds, trues
    else:
        model.eval()
        with torch.no_grad():
            for xb, yb in loader:
                xb = xb.to(DEVICE)
                p = model(xb).cpu().numpy()
                preds.append(p)
                trues.append(yb.numpy())
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        return preds, trues

def compute_point_metrics(y_true, y_pred):
    rmse = math.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    return {"rmse": rmse, "mae": mae}

# ---------------------------
# Optuna objective
# ---------------------------
def objective(trial, X_train, y_train, X_val, y_val):
    # hyperparameters to tune
    hidden_size = trial.suggest_categorical("hidden_size", [32, 64, 128])
    num_layers = trial.suggest_int("num_layers", 1, 3)
    dropout = trial.suggest_float("dropout", 0.05, 0.5)
    lr = trial.suggest_loguniform("lr", 1e-4, 1e-2)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])
    fc_units = trial.suggest_categorical("fc_units", [32, 64, 128])
    seq_len = trial.suggest_categorical("seq_len", [14, 21, 30, 60])

    # recreate sequences if seq_len differs
    Xt, yt = create_sequences(X_train, seq_len=seq_len, horizon=1)
    Xv, yv = create_sequences(X_val, seq_len=seq_len, horizon=1)

    train_ds = TimeSeriesDataset(Xt, yt)
    val_ds = TimeSeriesDataset(Xv, yv)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

    model = LSTM_MC_Dropout(input_size=X_train.shape[1],
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            dropout=dropout,
                            fc_units=fc_units).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    # simple early stopping
    best_val = float("inf")
    patience = 6
    epochs_no_improve = 0
    for epoch in range(1, 101):
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)
        # validate using deterministic forward pass (single)
        preds_val, trues_val = evaluate_model(model, val_loader, mc_dropout_samples=1)
        val_rmse = math.sqrt(mean_squared_error(trues_val, preds_val))
        trial.report(val_rmse, epoch)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
        if val_rmse < best_val - 1e-6:
            best_val = val_rmse
            epochs_no_improve = 0
            # store best weights
            best_state = model.state_dict()
        else:
            epochs_no_improve += 1
        if epochs_no_improve >= patience:
            break

    # load best
    model.load_state_dict(best_state)
    return best_val

# ---------------------------
# Main workflow
# ---------------------------
def main():
    # 1) Download dataset (Kaggle)
    try:
        download_kaggle_dataset(KAGGLE_DATASET, DATA_DIR)
    except Exception as e:
        print("Warning: Kaggle download step failed or skipped. Make sure dataset CSV is in ./data/ . Error:", e)

    # 2) Load
    df = load_daily_delhi_climate(DATA_DIR)

    # Choose target variable as first column (e.g., meantemp). Keep multivariate features.
    # We predict 1-step ahead of the first column.
    target_col = df.columns[0]
    print("Target column:", target_col)
    values = df.values  # shape (T, n_features)

    # 3) Train/Val/Test split (time-based)
    T = len(values)
    test_size = int(0.15 * T)
    val_size = int(0.15 * T)
    train_size = T - val_size - test_size
    train_vals = values[:train_size]
    val_vals = values[train_size:train_size+val_size]
    test_vals = values[train_size+val_size:]

    print(f"Splits -> train: {train_vals.shape[0]}, val: {val_vals.shape[0]}, test: {test_vals.shape[0]}")

    # 4) Scaling (fit on train)
    scaler = StandardScaler()
    scaler.fit(train_vals)
    train_scaled = scaler.transform(train_vals)
    val_scaled = scaler.transform(val_vals)
    test_scaled = scaler.transform(test_vals)

    # 5) Optuna hyperparameter search
    def optuna_objective(trial):
        return objective(trial, train_scaled, train_scaled, val_scaled, val_scaled)

    study = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=SEED),
                                pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))
    print("Starting Optuna tuning (this may take a while). Trials: 30")
    study.optimize(optuna_objective, n_trials=30, timeout=None, n_jobs=1)

    print("Optuna best trial:")
    print(study.best_trial.params)
    best_params = study.best_trial.params

    # 6) Build final model with best params, train on train+val combined, evaluate on test
    # Recreate full training set (train + val)
    combined_train = np.vstack([train_scaled, val_scaled])
    # Sequence length chosen by Optuna
    seq_len = best_params.get("seq_len", 30)

    X_train_seq, y_train_seq = create_sequences(combined_train, seq_len=seq_len, horizon=1)
    X_test_seq, y_test_seq = create_sequences(test_scaled, seq_len=seq_len, horizon=1)

    train_ds = TimeSeriesDataset(X_train_seq, y_train_seq)
    test_ds = TimeSeriesDataset(X_test_seq, y_test_seq)

    batch_size = best_params.get("batch_size", 32)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)

    model = LSTM_MC_Dropout(input_size=combined_train.shape[1],
                            hidden_size=best_params.get("hidden_size", 64),
                            num_layers=best_params.get("num_layers", 2),
                            dropout=best_params.get("dropout", 0.2),
                            fc_units=best_params.get("fc_units", 64)).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=best_params.get("lr", 1e-3))
    loss_fn = nn.MSELoss()

    # Train final model with early stopping
    best_val_loss = float("inf")
    patience = 8
    epochs_no_improve = 0
    for epoch in range(1, 201):
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)
        # Evaluate on train to monitor (no separate val now)
        model.eval()
        with torch.no_grad():
            preds_trn, trues_trn = evaluate_model(model, train_loader, mc_dropout_samples=1)
            trn_rmse = math.sqrt(mean_squared_error(trues_trn, preds_trn))
        # simple checkpointing by train loss
        if trn_rmse < best_val_loss - 1e-6:
            best_val_loss = trn_rmse
            epochs_no_improve = 0
            best_state = model.state_dict()
        else:
            epochs_no_improve += 1
        if epochs_no_improve >= patience:
            break
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: train_rmse={trn_rmse:.4f}")

    model.load_state_dict(best_state)
    print("Final model trained.")

    # 7) Evaluate with Monte Carlo Dropout
    MC_SAMPLES = 200  # number of stochastic forward passes
    preds_mc, trues = evaluate_model(model, test_loader, mc_dropout_samples=MC_SAMPLES)
    # preds_mc shape: (N, MC_SAMPLES)
    pred_mean = preds_mc.mean(axis=1)
    pred_std = preds_mc.std(axis=1)
    # 95% prediction intervals via empirical quantiles
    lower = np.percentile(preds_mc, 2.5, axis=1)
    upper = np.percentile(preds_mc, 97.5, axis=1)

    # Unscale predictions and trues (we only predicted the first column which was scaled)
    # Need to invert scaling for single-feature predictions: use scaler.mean_[0], scaler.scale_[0]
    def invert_scale(y_scaled):
        return (y_scaled * scaler.scale_[0]) + scaler.mean_[0]

    y_true_unscaled = invert_scale(trues)
    y_mean_unscaled = invert_scale(pred_mean)
    lower_unscaled = invert_scale(lower)
    upper_unscaled = invert_scale(upper)

    # Compute point metrics
    metrics_lstm = compute_point_metrics(y_true_unscaled, y_mean_unscaled)
    print("LSTM + MC Dropout test metrics (point forecast):", metrics_lstm)

    # Compute interval coverage (how many true values fall within 95% PI)
    coverage = np.mean((y_true_unscaled >= lower_unscaled) & (y_true_unscaled <= upper_unscaled))
    avg_interval_width = np.mean(upper_unscaled - lower_unscaled)
    print(f"Prediction interval coverage (95% PI): {coverage*100:.2f}%  avg width: {avg_interval_width:.4f}")

    # 8) Baseline: SARIMA using pmdarima (univariate on target series)
    # Fit on combined_train target series (unscaled original target)
    # Extract original (unscaled) target for train+val and test
    full_target = df.iloc[:train_size+val_size+test_size, 0].values  # original target
    train_val_target = full_target[:train_size+val_size]
    test_target = full_target[train_size+val_size:]

    # Auto ARIMA
    print("Fitting auto_arima (SARIMA) for baseline (this can take some time)...")
    arima_model = pm.auto_arima(train_val_target, seasonal=True, m=7,  # weekly seasonality as example
                                trace=False, error_action='ignore', suppress_warnings=True,
                                stepwise=True, n_jobs=1, max_order=10)
    print("ARIMA order:", arima_model.order, "seasonal_order:", arima_model.seasonal_order)

    # Forecast horizon = len(test_target)
    sarima_preds = arima_model.predict(n_periods=len(test_target))
    metrics_sarima = compute_point_metrics(test_target, sarima_preds)
    print("SARIMA test metrics:", metrics_sarima)

    # 9) Plot results: point forecast + intervals
    plt.figure(figsize=(14,6))
    t_idx = np.arange(len(y_true_unscaled))
    plt.plot(t_idx, y_true_unscaled, label="True (test)")
    plt.plot(t_idx, y_mean_unscaled, label="LSTM mean")
    plt.fill_between(t_idx, lower_unscaled, upper_unscaled, alpha=0.3, label="LSTM 95% PI")
    # Align SARIMA predictions (they predict same horizon)
    sarima_preds_aligned = sarima_preds  # same length
    plt.plot(t_idx, sarima_preds_aligned, label="SARIMA forecast")
    plt.legend()
    plt.title("Test set: LSTM + MC Dropout vs SARIMA (point forecasts & PI)")
    plt.xlabel("Time index (test)")
    plt.ylabel(target_col)
    plt.tight_layout()
    plt.savefig("forecast_comparison.png", dpi=200)
    print("Saved plot: forecast_comparison.png")

    # 10) Save models & params
    torch.save(model.state_dict(), "lstm_mc_dropout_best.pth")
    arima_model.save("sarima_model.pkl")
    with open("optuna_best_params.json", "w") as f:
        json.dump(best_params, f, indent=2)

    # 11) Summary report text (brief)
    report = f"""
SUMMARY REPORT
--------------
Dataset: {KAGGLE_DATASET}
Target: {target_col}
Train/Val/Test sizes: {train_size}/{val_size}/{test_size}

Optuna best params:
{json.dumps(best_params, indent=2)}

LSTM + MC Dropout (MC samples={MC_SAMPLES}):
 - RMSE: {metrics_lstm['rmse']:.4f}
 - MAE:  {metrics_lstm['mae']:.4f}
 - 95% PI coverage: {coverage*100:.2f}%
 - Avg interval width: {avg_interval_width:.4f}

SARIMA baseline:
 - RMSE: {metrics_sarima['rmse']:.4f}
 - MAE:  {metrics_sarima['mae']:.4f}

Notes:
 - LSTM point forecast compared against SARIMA on same test period.
 - Uncertainty quantification via MC Dropout (Gal & Ghahramani 2016) approximates Bayesian posterior by performing multiple stochastic forward passes with dropout enabled during inference.
 - Calibration (coverage) indicates how well the prediction intervals contain observed values; ideal ~95% for 95% PI.

Files produced:
 - forecast_comparison.png
 - lstm_mc_dropout_best.pth
 - sarima_model.pkl
 - optuna_best_params.json
"""
    print(report)
    with open("summary_report.txt","w") as f:
        f.write(report)

if __name__ == "__main__":
    main()